<!DOCTYPE html>
<html lang="zh-CN">
<head>
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>
        
            与Word2vec和机器学习的第一次接触
        
    </title>
    <link rel="icon" href="/img/favicon.png"/>
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet"
      href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/styles/monokai-sublime.min.css">
<script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>

    
<link rel="stylesheet" href="/css/style.css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-161437833-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-161437833-1');
</script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="moonshine" type="application/atom+xml">
</head>
<body>
    <header class="header" id="header">
    <h1>
        <a class="title" href="/">
            moonshine
        </a>
    </h1>
    <h2>
        <a class="motto">
            Et la lune descend sur le temple qui fut.
        </a>
    </h2>
    <nav class="navbar">
        <ul class="menu">
            
            
                <li class="menu-item">
                    <a href="/" class="menu-item-link">
                        Home
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="/about/" class="menu-item-link">
                        About
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="/friends/" class="menu-item-link">
                        Friends
                    </a>
                </li>
            
                
            
                
                
        </ul>
    </nav>
</header>
    <main class="main">
        <article class="post">
            <h1>
                <a class="title" href="/2017/05/14/%E4%B8%8EWord2vec%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8E%A5%E8%A7%A6/"> 
                    与Word2vec和机器学习的第一次接触 
                </a>
            </h1>
            <div class="meta">
                <a class="date"> 
                    <i class="fa fa-calendar" aria-hidden="true"></i>                    
                    2017-05-14   
                </a>
                
                <a class="category">
                    <i class="fa fa-th" aria-hidden="true"></i>  
                </a>
               
                <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></li></ul>
                
                <a class="tag">
                    <i class="fa fa-tags" aria-hidden="true"></i>  
                </a>
                
                    <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A7%91%E7%A0%94/" rel="tag">科研</a></li></ul>
            </div>
<div class="toc">
  <ol class="toc-list"><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#1-实验背景"><span class="toc-list-text">1 实验背景</span></a></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#2-实验目标"><span class="toc-list-text">2 实验目标</span></a></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#3-过程概述"><span class="toc-list-text">3 过程概述</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#3-1-前期准备"><span class="toc-list-text">3.1 前期准备</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#3-1-1-运行环境"><span class="toc-list-text">3.1.1 运行环境</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#3-1-2-文本采集和分词：校园网全站"><span class="toc-list-text">3.1.2 文本采集和分词：校园网全站</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#3-1-3-确定分类依据：大文科类、大理科类"><span class="toc-list-text">3.1.3 确定分类依据：大文科类、大理科类</span></a></li></ol></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#3-2-实验简述"><span class="toc-list-text">3.2 实验简述</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#3-2-1-改进方案的思路：文章所有词向量的累加平均"><span class="toc-list-text">3.2.1 改进方案的思路：文章所有词向量的累加平均</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#3-2-2-主要方案介绍"><span class="toc-list-text">3.2.2 主要方案介绍</span></a></li></ol></li></ol></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#4-实验结果"><span class="toc-list-text">4 实验结果</span></a></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#5-结果分析"><span class="toc-list-text">5 结果分析</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#5-1-Word2Vec和TF-IDF-SVM和Naive-Bayes的对比"><span class="toc-list-text">5.1 Word2Vec和TF-IDF , SVM和Naive Bayes的对比</span></a></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#5-2-采用各种语料训练的Word2Vec模型对比"><span class="toc-list-text">5.2 采用各种语料训练的Word2Vec模型对比</span></a></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#5-3-未能在数据中直观体现的内容"><span class="toc-list-text">5.3 未能在数据中直观体现的内容</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-3-1-Word2Vec模型的可扩展性"><span class="toc-list-text">5.3.1 Word2Vec模型的可扩展性</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-3-2-不同维度对准确率的影响"><span class="toc-list-text">5.3.2 不同维度对准确率的影响</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-3-3-TF-IDF的特征利用率可能很低下"><span class="toc-list-text">5.3.3 TF-IDF的特征利用率可能很低下</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-3-4-Word2Vec的相关参数"><span class="toc-list-text">5.3.4 Word2Vec的相关参数</span></a></li></ol></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#5-4-一些猜想"><span class="toc-list-text">5.4 一些猜想</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-4-1-为何朴素贝叶斯分类器表现平平：不适应文本"><span class="toc-list-text">5.4.1 为何朴素贝叶斯分类器表现平平：不适应文本</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#5-4-2-为何Word2Vec比TF-IDF表现更好：模型更合理"><span class="toc-list-text">5.4.2 为何Word2Vec比TF-IDF表现更好：模型更合理</span></a></li></ol></li></ol></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#6-不足和展望"><span class="toc-list-text">6 不足和展望</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#6-1-不足的部分"><span class="toc-list-text">6.1 不足的部分</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#6-1-1-知识欠缺"><span class="toc-list-text">6.1.1 知识欠缺</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#6-1-2-资源不足"><span class="toc-list-text">6.1.2 资源不足</span></a></li></ol></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#6-2-接下来的任务"><span class="toc-list-text">6.2 接下来的任务</span></a></li></ol></li><li class="toc-list-item toc-list-level-1"><a class="toc-list-link" href="#7-附录"><span class="toc-list-text">7 附录</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#7-1-Word2Vec衍生试验"><span class="toc-list-text">7.1 Word2Vec衍生试验</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-1-1-语料的对模型的影响"><span class="toc-list-text">7.1.1 语料的对模型的影响</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-1-2-窗口的对模型的影响"><span class="toc-list-text">7.1.2 窗口的对模型的影响</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-1-3-验证Google的噱头"><span class="toc-list-text">7.1.3 验证Google的噱头</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-1-3-1-King-Women-Man-Queen"><span class="toc-list-text">7.1.3.1 King + Women - Man &#x3D; Queen</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-1-3-2-意义明确的例子：马云-企业家-互联网-王石"><span class="toc-list-text">7.1.3.2 意义明确的例子：马云 + 企业家 - 互联网 &#x3D; 王石</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-1-3-3-抽象的例子：神庙-工业-宗教-钢结构"><span class="toc-list-text">7.1.3.3 抽象的例子：神庙 + 工业 - 宗教 &#x3D; 钢结构</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-1-3-4-为什么会失败：火药桶-东亚-地区-欧洲-桥头堡"><span class="toc-list-text">7.1.3.4 为什么会失败：火药桶 + 东亚 + 地区 - 欧洲 &#x3D; 桥头堡</span></a></li></ol></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-1-4-扩充情感词典的试验"><span class="toc-list-text">7.1.4 扩充情感词典的试验</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-1-5-相似文本生成"><span class="toc-list-text">7.1.5 相似文本生成</span></a></li></ol></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#7-2-数据相关"><span class="toc-list-text">7.2 数据相关</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-2-1-爬虫"><span class="toc-list-text">7.2.1 爬虫</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-2-2-文本排重"><span class="toc-list-text">7.2.2 文本排重</span></a></li><li class="toc-list-item toc-list-level-3"><a class="toc-list-link" href="#7-2-3-分类情况"><span class="toc-list-text">7.2.3 分类情况</span></a><ol class="toc-list-child"><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-2-3-1-校内大理科类"><span class="toc-list-text">7.2.3.1 校内大理科类</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-2-3-2-校内大文科类"><span class="toc-list-text">7.2.3.2 校内大文科类</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-2-3-3-外校大文科类"><span class="toc-list-text">7.2.3.3 外校大文科类</span></a></li><li class="toc-list-item toc-list-level-4"><a class="toc-list-link" href="#7-2-3-4-外校大理科类"><span class="toc-list-text">7.2.3.4 外校大理科类</span></a></li></ol></li></ol></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#7-3-有关Word2Vec应用的中文论文"><span class="toc-list-text">7.3 有关Word2Vec应用的中文论文</span></a></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#7-4-未列出的实验内容"><span class="toc-list-text">7.4 未列出的实验内容</span></a></li><li class="toc-list-item toc-list-level-2"><a class="toc-list-link" href="#7-5-所用模型和算法的实现"><span class="toc-list-text">7.5 所用模型和算法的实现</span></a></li></ol></li></ol>
</div>
            <div class="content">
                <h1 id="1-实验背景"><a href="#1-实验背景" class="headerlink" title="1 实验背景"></a>1 实验背景</h1><p>&emsp;&emsp;阅读论文《面向软件缺陷报告的提取方法》后，我意识到，需要丰富机器学习和数据挖掘方面的知识，并形成用它们解决问题的思维方式。<br>&emsp;&emsp;作者林涛指出：使用高维特征向量的支持向量机“<strong>很容易在训练集中取得很好的效果，但在测试集外效果不理想</strong>”。通过查阅资料，可以发现<strong>这一方向有新的进展</strong>。<br>&emsp;&emsp;Google提出了基于词向量的特征提取方式（Word2Vec），并发布了基于海量新闻数据训练的词向量模型。由于高效、准确，Word2Vec在英语环境中已经成为文本分类的学术热点。目前，面向英语的Word2Vec的分支已经出现了用于消歧义的、可标注词性的词向量工具Sense2Vec。在内容分类领域，出现了使用类似模型实现降维的Image2Vec和Video2Vec等学术成果，前景广阔。<br>&emsp;&emsp;Word2Vec在中文的应用也有不俗的效果，尤其是在特定领域的文本分类、词典扩充等方面（见附录7.3），然而词向量在中文方面的基础数据和工具仍然在建设阶段。</p>
<h1 id="2-实验目标"><a href="#2-实验目标" class="headerlink" title="2 实验目标"></a>2 实验目标</h1><ol>
<li>尝试论文中推荐的<strong>朴素贝叶斯（Naive Bayes）分类器</strong></li>
<li>尝试论文中用于对照的<strong>支持向量机（SVM）分类器</strong></li>
<li>从<strong>改变特征向量提取方式</strong>的角度出发探索改进方案</li>
<li>利用以上方案进行<strong>文本二元分类</strong>的实验</li>
</ol>
<h1 id="3-过程概述"><a href="#3-过程概述" class="headerlink" title="3 过程概述"></a>3 过程概述</h1><h2 id="3-1-前期准备"><a href="#3-1-前期准备" class="headerlink" title="3.1 前期准备"></a>3.1 前期准备</h2><h3 id="3-1-1-运行环境"><a href="#3-1-1-运行环境" class="headerlink" title="3.1.1 运行环境"></a>3.1.1 运行环境</h3><p>&emsp;&emsp;本次实验的系统环境为 <strong>Python 3.6.1 , Mac OS X 10.12</strong> , 2.3 GHz Intel Core i7 , 16 GB 1600 MHz DDR3，并采用 PyCharm 作为集成开发环境。</p>
<h3 id="3-1-2-文本采集和分词：校园网全站"><a href="#3-1-2-文本采集和分词：校园网全站" class="headerlink" title="3.1.2 文本采集和分词：校园网全站"></a>3.1.2 文本采集和分词：校园网全站</h3><p>&emsp;&emsp;本次实验用爬虫从上海师范大学的校园网（<a href="http://www.shnu.edu.cn/" target="_blank" rel="noopener">http://www.shnu.edu.cn/</a>) 及其公开子域采集了36万篇文章，经过初步排重，共计有效文章<strong>约24万篇</strong>，<strong>作为Word2Vec词向量模型的训练数据</strong>。</p>
<p>&emsp;&emsp;经过排重后，需要对文本进行分词，为输入特征提取工具做准备。</p>
<h3 id="3-1-3-确定分类依据：大文科类、大理科类"><a href="#3-1-3-确定分类依据：大文科类、大理科类" class="headerlink" title="3.1.3 确定分类依据：大文科类、大理科类"></a>3.1.3 确定分类依据：大文科类、大理科类</h3><p>&emsp;&emsp;为了保证数据分类的客观性，本次实验未采用手工标注数据的方式，而是将<strong>人文社科、艺术类学院</strong>对应子域下的文章分为大文科类，将<strong>理工商科学院</strong>对应子域下的文章分为大理科类。</p>
<p>&emsp;&emsp;符合以上标准的有效数据共计43483篇，其中大文科类30908篇，大理科类12575篇。在此基础上，<strong>随机抽取80%的数据用作训练，剩余20%的数据用作测试</strong>。</p>
<p>&emsp;&emsp;为了分析和验证林涛在《面向软件缺陷报告的提取方法》论文中提到的<strong>支持向量机的缺陷</strong>，本次实验另从<strong>外校的对应学院</strong>抓取了额外的测试数据，其中大文科类1810篇、大理科类3283篇。</p>
<p>&emsp;&emsp;数据分类的详情在附录7.2.3中有另外说明。</p>
<h2 id="3-2-实验简述"><a href="#3-2-实验简述" class="headerlink" title="3.2 实验简述"></a>3.2 实验简述</h2><h3 id="3-2-1-改进方案的思路：文章所有词向量的累加平均"><a href="#3-2-1-改进方案的思路：文章所有词向量的累加平均" class="headerlink" title="3.2.1 改进方案的思路：文章所有词向量的累加平均"></a>3.2.1 改进方案的思路：文章所有词向量的累加平均</h3><p>&emsp;&emsp;支持向量机需要面向文章的特征向量进行分类。若要将Word2Vec与其结合，就需要基于词向量计算文章的特征向量。<br>&emsp;&emsp;本次实验采用的方式是<strong>对每篇文章进行分词，在词向量模型中搜索词语对应的特征向量，并对搜索结果进行累加平均</strong>。<br>&emsp;&emsp;对于中文，可能存在更合理的、基于语义和语法的加权平均方式，但未找到相关论文。    </p>
<h3 id="3-2-2-主要方案介绍"><a href="#3-2-2-主要方案介绍" class="headerlink" title="3.2.2 主要方案介绍"></a>3.2.2 主要方案介绍</h3><p>实验中获得较好成绩的有以下四种方案（表一）</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>特征提取工具</th>
<th>提取特征所用语料</th>
<th>分类器</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>Word2Vec(300维)</td>
<td>上师大全站文本</td>
<td>支持向量机</td>
<td>3.2.1提出的改进方案</td>
</tr>
<tr>
<td>B</td>
<td>Word2Vec(400维)</td>
<td>搜狐新闻单月数据</td>
<td>支持向量机</td>
<td>与A对比-特征语料的类型</td>
</tr>
<tr>
<td>C</td>
<td>Word2Vec(300维)</td>
<td>校内数据训练集</td>
<td>支持向量机</td>
<td>与A对比-特征语料的数量</td>
</tr>
<tr>
<td>D</td>
<td>TF-IDF</td>
<td>校内数据训练集</td>
<td>支持向量机</td>
<td>与C对比-特征提取工具，文本分类的主流方案</td>
</tr>
<tr>
<td>E</td>
<td>TF-IDF</td>
<td>校内数据训练集</td>
<td>朴素贝叶斯</td>
<td>与C、D对比-分类器，林涛论文的方案</td>
</tr>
</tbody></table>
<p>注1：TF-IDF采用全网语料，几乎未对成绩产生影响，将于5.3.3讨论。<br>注2：朴素贝叶斯和Word2Vec结合效果不佳，未列出。</p>
<h1 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h1><p>校内数据测试结果（表二）</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>测试数据</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>校内数据测试集</td>
<td>0.96</td>
<td>0.95</td>
<td>0.95</td>
<td>改进方案</td>
</tr>
<tr>
<td>B</td>
<td>校内数据测试集</td>
<td>0.89</td>
<td>0.88</td>
<td>0.87</td>
<td>不同语料种类对比</td>
</tr>
<tr>
<td>C</td>
<td>校内数据测试集</td>
<td>0.95</td>
<td>0.95</td>
<td>0.95</td>
<td>不同语料数量对比，训练数据为A的子集</td>
</tr>
<tr>
<td>D</td>
<td>校内数据测试集</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>特征提取工具对比</td>
</tr>
<tr>
<td>E</td>
<td>校内数据测试集</td>
<td>0.89</td>
<td>0.88</td>
<td>0.87</td>
<td>分类器对比</td>
</tr>
</tbody></table>
<p>校外数据测试结果（表三）</p>
<table>
<thead>
<tr>
<th>方案</th>
<th>测试数据</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>外校数据</td>
<td>0.90</td>
<td>0.90</td>
<td>0.90</td>
<td>改进方案</td>
</tr>
<tr>
<td>B</td>
<td>外校数据</td>
<td>0.79</td>
<td>0.71</td>
<td>0.72</td>
<td>不同语料种类对比</td>
</tr>
<tr>
<td>C</td>
<td>外校数据</td>
<td>0.86</td>
<td>0.86</td>
<td>0.86</td>
<td>不同语料数量对比，训练数据为A的子集</td>
</tr>
<tr>
<td>D</td>
<td>外校数据</td>
<td>0.81</td>
<td>0.75</td>
<td>0.76</td>
<td>特征提取工具对比</td>
</tr>
<tr>
<td>E</td>
<td>外校数据</td>
<td>0.80</td>
<td>0.78</td>
<td>0.76</td>
<td>分类器对比</td>
</tr>
</tbody></table>
<p>方案A-校内数据-详情（表四）</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>样本数</th>
</tr>
</thead>
<tbody><tr>
<td>大文科</td>
<td>0.94</td>
<td>0.99</td>
<td>0.97</td>
<td>6123</td>
</tr>
<tr>
<td>大理科</td>
<td>0.98</td>
<td>0.86</td>
<td>0.92</td>
<td>2574</td>
</tr>
<tr>
<td>平均/总量</td>
<td>0.96</td>
<td>0.95</td>
<td>0.95</td>
<td>8697</td>
</tr>
</tbody></table>
<p>方案A-校外数据-详情（表五）</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>样本数</th>
</tr>
</thead>
<tbody><tr>
<td>大文科</td>
<td>0.85</td>
<td>0.86</td>
<td>0.85</td>
<td>1810</td>
</tr>
<tr>
<td>大理科</td>
<td>0.92</td>
<td>0.91</td>
<td>0.92</td>
<td>3283</td>
</tr>
<tr>
<td>平均/总量</td>
<td>0.90</td>
<td>0.90</td>
<td>0.90</td>
<td>5093</td>
</tr>
</tbody></table>
<p>方案D-校内数据-详情（表六）</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>样本数</th>
</tr>
</thead>
<tbody><tr>
<td>大文科</td>
<td>0.99</td>
<td>1.00</td>
<td>0.99</td>
<td>6123</td>
</tr>
<tr>
<td>大理科</td>
<td>0.99</td>
<td>0.98</td>
<td>0.98</td>
<td>2574</td>
</tr>
<tr>
<td>平均/总量</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>8697</td>
</tr>
</tbody></table>
<p>方案D-校外数据-详情（表七）</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>准确率</th>
<th>召回率</th>
<th>F-Score</th>
<th>样本数</th>
</tr>
</thead>
<tbody><tr>
<td>大文科</td>
<td>0.60</td>
<td>0.91</td>
<td>0.72</td>
<td>1810</td>
</tr>
<tr>
<td>大理科</td>
<td>0.93</td>
<td>0.66</td>
<td>0.77</td>
<td>3283</td>
</tr>
<tr>
<td>平均/总量</td>
<td>0.81</td>
<td>0.75</td>
<td>0.76</td>
<td>5093</td>
</tr>
</tbody></table>
<h1 id="5-结果分析"><a href="#5-结果分析" class="headerlink" title="5 结果分析"></a>5 结果分析</h1><h2 id="5-1-Word2Vec和TF-IDF-SVM和Naive-Bayes的对比"><a href="#5-1-Word2Vec和TF-IDF-SVM和Naive-Bayes的对比" class="headerlink" title="5.1 Word2Vec和TF-IDF , SVM和Naive Bayes的对比"></a>5.1 Word2Vec和TF-IDF , SVM和Naive Bayes的对比</h2><p>&emsp;&emsp;通过<strong>对比方案C、D和E</strong>，可以得出结论：<strong>基于Word2Vec和SVM的方案C优于方案D、E</strong>。<br>&emsp;&emsp;分类器方面,C和D的数据在表二和表三都优于E，<strong>支持向量机在文本分类任务中的表现优于朴素贝叶斯</strong>。<br>&emsp;&emsp;特征提取工具方面，在校内的测试数据中，基于TF-IDF的方案D获得了让人惊讶的成绩，基于Word2Vec的方案C也没有落后太多。<br>&emsp;&emsp;但方案D显然是不够稳定的，它在校外数据里表现不好。通过表七内D的详细数据，可以发现它出现了经典的矛盾：<strong>高召回率和高准确率通常难以兼顾</strong>。<br>&emsp;&emsp;与之相比，C面对外部数据表现出的劣化就要小很多。<br>&emsp;&emsp;<strong>以上的数据可能从一个侧面验证了林涛的说法</strong>，SVM+TF-IDF的数据可以跑得漂亮，但难以作为一个普适的方案适应更多文本。</p>
<h2 id="5-2-采用各种语料训练的Word2Vec模型对比"><a href="#5-2-采用各种语料训练的Word2Vec模型对比" class="headerlink" title="5.2 采用各种语料训练的Word2Vec模型对比"></a>5.2 采用各种语料训练的Word2Vec模型对比</h2><p>&emsp;&emsp;通过<strong>对比方案A、B和C</strong>，可以得出结论：<strong>选择特定领域的、尽可能多的文本有助于进一步提高准确率，方案A是最优的</strong>。<br>&emsp;&emsp;使用了上师大全网文本的模型成绩最好，部分学院文本的模型次之，搜狐新闻的文本模型最次。<br>&emsp;&emsp;搜狐新闻的文本最多，但相关性不如另两个，说明数量大并不一定可以带来好的结果。<br>&emsp;&emsp;部分学院文本在校内数据的成绩与全网文本几乎一致，但分类校外数据就被拉开了差距，说明合适文本的数量仍然是需要重视的因素。</p>
<h2 id="5-3-未能在数据中直观体现的内容"><a href="#5-3-未能在数据中直观体现的内容" class="headerlink" title="5.3 未能在数据中直观体现的内容"></a>5.3 未能在数据中直观体现的内容</h2><h3 id="5-3-1-Word2Vec模型的可扩展性"><a href="#5-3-1-Word2Vec模型的可扩展性" class="headerlink" title="5.3.1 Word2Vec模型的可扩展性"></a>5.3.1 Word2Vec模型的可扩展性</h3><p>&emsp;&emsp;在进行累加平均计算时，可以进行模型的”热插拔”，即使用训练好的SVM分类器和任意的Word2Vec模型。<br>&emsp;&emsp;如果没有时间训练模型，甚至可以使用现成的、综合内容的模型数据直接训练分类器，符合当下产品迭代升级的节奏和思路。<br>&emsp;&emsp;Word2Vec模型训练的效率和速度在单机上也很理想，并且可以保存进度，<strong>进行增量训练</strong>。<br>&emsp;&emsp;对比<strong>TF-IDF每次分类都需要重新基于训练集计算特征</strong>的情况，Word2Vec更实用。         </p>
<h3 id="5-3-2-不同维度对准确率的影响"><a href="#5-3-2-不同维度对准确率的影响" class="headerlink" title="5.3.2 不同维度对准确率的影响"></a>5.3.2 不同维度对准确率的影响</h3><p>&emsp;&emsp;在对相同语料尝试了300、400这两个维度之后，发现F-score在[-0.01,0.02]之间小幅波动，本次实验<strong>未能得出和维度相关的一般规律</strong>。<br>&emsp;&emsp;但在尝试1500维时，在校外数据中F-score突然下降到0.5附近，主要体现为准确率极低。<br>&emsp;&emsp;1500是远超官方推荐维度数量的值，说明矩阵过于稀疏对结果会产生很大的负面影响。<br>&emsp;&emsp;这些影响的内在规律需要需要进一步实验确定。    </p>
<h3 id="5-3-3-TF-IDF的特征利用率可能很低下"><a href="#5-3-3-TF-IDF的特征利用率可能很低下" class="headerlink" title="5.3.3 TF-IDF的特征利用率可能很低下"></a>5.3.3 TF-IDF的特征利用率可能很低下</h3><p>&emsp;&emsp;在扩大了语料之后，以Word2Vec为特征提取工具的SVM分类器方案在两类测试数据中获得了很大的进步。<br>&emsp;&emsp;但当使用了全网数据，使得TF-IDF的feature数从271225扩大为519812之后，SVM和Naive Bayes两个分类器的测试成绩几乎都没有变化，甚至还有轻微倒退。<br>&emsp;&emsp;这不得不让人质疑，增加的近一倍的feature为何无效。<br>&emsp;&emsp;但作为传统惯用的特征提取工具，TF-IDF已经被广泛证明为高效可用的方案之一，这个问题仍需进一步调查。    </p>
<h3 id="5-3-4-Word2Vec的相关参数"><a href="#5-3-4-Word2Vec的相关参数" class="headerlink" title="5.3.4 Word2Vec的相关参数"></a>5.3.4 Word2Vec的相关参数</h3><p>&emsp;&emsp;本次实验中，Word2Vec的词向量均采用5的窗口(window)值，该值的选用需要参考语料的句子长度，因为Word2Vec所用Skip-Gram算法的特性，窗口过大会影响准确率。详情可见附录7.1.2。</p>
<p>&emsp;&emsp;<strong>难以明确每一维的实际含义是词向量为人诟病的问题之一</strong>。方案A中，Word2Vec选用了300维度。方案B中，Word2Vec选用了400维度。维数的选定缺乏权威资料，官方文档推荐选择数十到数百，维度大小要参考语料的数量。搜狐新闻的语料数量更大，于是采用了更大的维度。</p>
<h2 id="5-4-一些猜想"><a href="#5-4-一些猜想" class="headerlink" title="5.4 一些猜想"></a>5.4 一些猜想</h2><p>&emsp;&emsp;基于分类器和特征提取工具的<strong>工作原理</strong>和<strong>文本特征</strong>推断，未必准确，也难以验证。    </p>
<h3 id="5-4-1-为何朴素贝叶斯分类器表现平平：不适应文本"><a href="#5-4-1-为何朴素贝叶斯分类器表现平平：不适应文本" class="headerlink" title="5.4.1 为何朴素贝叶斯分类器表现平平：不适应文本"></a>5.4.1 为何朴素贝叶斯分类器表现平平：不适应文本</h3><p>&emsp;&emsp;朴素贝叶斯分类器是<strong>多类别分类器</strong>。朴素贝叶斯基于先验概率推测后验概率，再用后验概率推测可能的分类，它假设事件之间都是独立的，因此分类器适合分类独立的特征，例如门户网站的新闻类别。<br>&emsp;&emsp;本次实验分类和文本之间的独立性不强，首先，文本都是学院的文稿而非纯学科学术的文本；其次，学科之间交叉的情况也很多，例如”计算语言学”。    </p>
<h3 id="5-4-2-为何Word2Vec比TF-IDF表现更好：模型更合理"><a href="#5-4-2-为何Word2Vec比TF-IDF表现更好：模型更合理" class="headerlink" title="5.4.2 为何Word2Vec比TF-IDF表现更好：模型更合理"></a>5.4.2 为何Word2Vec比TF-IDF表现更好：模型更合理</h3><p>&emsp;&emsp;TF-IDF对词频非常敏感，低频的、在更多文档中出现的词在决定文档类型方面有更大的权重，这无疑是合理的。<br>&emsp;&emsp;Word2Vec的词向量是语言模型生成中的副产物，简而言之，通过推断A+B=C和A+D=C都是合理的，从而产生B和D相似的结论，在这个过程中，词频也是一个考虑因素。<br>&emsp;&emsp;通过增多维度数量，哪怕难以说清每一维的具体含义，它也能达到类似表达语义的效果，因为每一维可以被看作在有限文本中机器所推测出的语义。<br>&emsp;&emsp;决定一篇文本含义的根本特征应该是语义，而不是词频。所以并不是因为TF-IDF不适应文本，而是因为词向量表示文本的特征更合理，它可以表达更多的信息。<br>&emsp;&emsp;因此随着文本量的提升，词向量的表现也会随之得到改善。<br>&emsp;&emsp;根据这个推测，词向量在大量文本中的表现应该优于词频模型，这就需要大量实验来支撑了。    </p>
<h1 id="6-不足和展望"><a href="#6-不足和展望" class="headerlink" title="6 不足和展望"></a>6 不足和展望</h1><p>&emsp;&emsp;和林涛的邮件讨论中，他肯定了我的实验过程，也提出了改进的建议。结合我自身的认识，在此予以列出。</p>
<h2 id="6-1-不足的部分"><a href="#6-1-不足的部分" class="headerlink" title="6.1 不足的部分"></a>6.1 不足的部分</h2><h3 id="6-1-1-知识欠缺"><a href="#6-1-1-知识欠缺" class="headerlink" title="6.1.1 知识欠缺"></a>6.1.1 知识欠缺</h3><ol>
<li>Word2Vec的Skip-Gram提出的模型是深度学习的范畴，但未将深度学习方面在分类任务中表现出色的卷积神经网络模型（CNN）作为分类器加入实验。</li>
<li>缺乏对Word2Vec具体实现的细致认识，调参较为盲目。</li>
<li>根据词向量计算文章的特征向量的方式有改进空间。</li>
<li>未能就特定方向的任务（例如软件缺陷报告）对现有的开源实现进行横向对比。例如基于支持向量机的软件缺陷报告提取实现：<a href="https://www.cs.ubc.ca/cs-research/software-practices-lab/projects/summarizing-software-artifacts" target="_blank" rel="noopener">https://www.cs.ubc.ca/cs-research/software-practices-lab/projects/summarizing-software-artifacts</a></li>
</ol>
<h3 id="6-1-2-资源不足"><a href="#6-1-2-资源不足" class="headerlink" title="6.1.2 资源不足"></a>6.1.2 资源不足</h3><ol>
<li>限于设备算力、资源和精力，在本次实验中未能使用规模更大的、更具有现实意义的、更具有一般性和高质量的语料。</li>
<li>处理中文语料需要分词，本次实验未采用自定义词库。</li>
</ol>
<h2 id="6-2-接下来的任务"><a href="#6-2-接下来的任务" class="headerlink" title="6.2 接下来的任务"></a>6.2 接下来的任务</h2><ol>
<li>在二元分类的基础上，实现文本的多类别分类。</li>
<li>限定某个领域，对该领域的语料进行精准的分词和挖掘，并扩充语料库、提高语料质量。</li>
<li>将词向量输入更多的分类器，如CNN。</li>
<li>试验用无监督的方式解决此类问题的准确性，例如K-Means聚类或LDA聚类。    </li>
</ol>
<h1 id="7-附录"><a href="#7-附录" class="headerlink" title="7 附录"></a>7 附录</h1><h2 id="7-1-Word2Vec衍生试验"><a href="#7-1-Word2Vec衍生试验" class="headerlink" title="7.1 Word2Vec衍生试验"></a>7.1 Word2Vec衍生试验</h2><h3 id="7-1-1-语料的对模型的影响"><a href="#7-1-1-语料的对模型的影响" class="headerlink" title="7.1.1 语料的对模型的影响"></a>7.1.1 语料的对模型的影响</h3><p>&emsp;&emsp;对比三个词向量模型对于同一个词的不同近似词输出，相似度从左到右递减。这三个模型的语料分别来自上师大校园网、搜狐新闻和一万多篇程序员博客。  </p>
<p>可以<strong>得出结论</strong>：    </p>
<ol>
<li>如果需要分析的语料<strong>具有领域的特性</strong>，要尽量<strong>在对应的语料上训练</strong>。   </li>
<li>如果要分析的语料<strong>难以确定特性</strong>，需要探索<strong>消歧义</strong>的方式才能提高精确度。</li>
</ol>
<p>结果：</p>
<p>上海师大校园网 - 驱动：<br>内生 促进改革 原动力 导向 战略     </p>
<p>搜狐新闻 - 驱动：<br>牵引 驱动力 耦合 转变 结构 后驱      </p>
<p>程序员博客 - 驱动：<br>显卡 强力 声卡 ATI 驱动程序     </p>
<hr>
<p>上海师大校园网 - 资源：<br>教育资源 公共资源 人才资源 资源优势 网络资源 资源整合 自然资源 教学资源 客户资源 集约     </p>
<p>搜狐新闻 - 资源：<br>资源优势 教育资源 人才资源 网络资源 矿产资源 自然资源 煤炭资源 海洋资源 水资源 地热资源     </p>
<p>程序员博客 - 资源：<br>系统资源 缓存 CDN 存储空间 资金 外链 内存 延迟 策略 开销     </p>
<h3 id="7-1-2-窗口的对模型的影响"><a href="#7-1-2-窗口的对模型的影响" class="headerlink" title="7.1.2 窗口的对模型的影响"></a>7.1.2 窗口的对模型的影响</h3><p>&emsp;&emsp;在大多数情况下，5和15的window值只会导致结果的前后位置变动，但一般都是正确答案的范畴。<br>&emsp;&emsp;然而，不合理的数值总会露出马脚。<br>&emsp;&emsp;根据Skip-Gram模型的特性，它会以当前词为基点，<strong>计算前后window*2个词语和该词语的关系</strong>。<br>&emsp;&emsp;训练时，本人将文本以篇为单位输入模型，所以并没有根据句子切分。window值太大理论上会<strong>导致文本存在噪音</strong>。<br>&emsp;&emsp;通常来说，<strong>中文的一句话有11个词是合理的，但31个词就是长句了</strong>。<br>&emsp;&emsp;实测下来，<strong>窗口15的模型经常会把连续的词语当作相似度高的词输出</strong>，而正解会被排到后面，例如：    </p>
<p>窗口5 - 夺得：<br>摘得 勇夺 拿下 桂冠 斩获<br>窗口15 - 夺得：<br>摘得 桂冠 勇夺 一举 拿下    </p>
<p>窗口5 - 转折点：<br>转折 里程碑 缩影 黄金时代 苦短<br>窗口15 - 转折点：<br>转折 此战 奇迹 里程碑 史上     </p>
<p>窗口5 - 振聋发聩：<br>掷地有声 一句 刺耳 大公无私 哽咽<br>窗口15 - 振聋发聩：<br>流芳百世 无以 不掉 发人深省 然 不由 掷地有声    </p>
<p>&emsp;&emsp;<strong>在维度不变的情况下，单个词需要考虑的特征越多，结果会越不精确，是说得通的。今后还需要进一步试验，在维度和window都提升的情况下，效果会如何</strong>。     </p>
<h3 id="7-1-3-验证Google的噱头"><a href="#7-1-3-验证Google的噱头" class="headerlink" title="7.1.3 验证Google的噱头"></a>7.1.3 验证Google的噱头</h3><h4 id="7-1-3-1-King-Women-Man-Queen"><a href="#7-1-3-1-King-Women-Man-Queen" class="headerlink" title="7.1.3.1 King + Women - Man = Queen"></a>7.1.3.1 King + Women - Man = Queen</h4><p>&emsp;&emsp;Google在推出新产品时往往会有一个抓眼球的噱头。<br>&emsp;&emsp;这个噱头往往可以实现，但并没有传说的那么神奇和实用。<br>&emsp;&emsp;虽然词向量更大的作用体现在预处理数据上，但是这次，Word2Vec也就有一个广为传播的噱头：King + Women - Man = Queen    </p>
<p>&emsp;&emsp;它是很容易复现的。    </p>
<blockquote>
<p>word_vectors.most_similar(positive=[‘国王’,’女性’],negative=[‘男性’])   </p>
</blockquote>
<p>&emsp;&emsp;以上代码相当于计算”国王+女性-男性”</p>
<blockquote>
<p>[(‘王后’, 0.582228422164917), (‘王储’, 0.5397378206253052), (‘五世’, 0.5332286357879639), (‘二世’, 0.5186830759048462), (‘王室’, 0.5071204900741577), (‘叶卡捷琳娜’, 0.5062432885169983)]</p>
</blockquote>
<p>&emsp;&emsp;以上的返回值是一个list，它包含了若干(key,similarity)的tuple。<br>&emsp;&emsp;可以看到<strong>相似度最高的是王后</strong>。</p>
<p>&emsp;&emsp;但值得注意的是，如果<strong>把女性和男性换成女和男</strong>，准度就会大大降低。<br>&emsp;&emsp;推测有两个方面的原因。<br>&emsp;&emsp;第一，中文的单字的精度表现往往会差一些。<br>&emsp;&emsp;第二，女和女性的相似度由于数据量的问题所以并没有被训练到理想程度。      </p>
<hr>
<h4 id="7-1-3-2-意义明确的例子：马云-企业家-互联网-王石"><a href="#7-1-3-2-意义明确的例子：马云-企业家-互联网-王石" class="headerlink" title="7.1.3.2 意义明确的例子：马云 + 企业家 - 互联网 = 王石"></a>7.1.3.2 意义明确的例子：马云 + 企业家 - 互联网 = 王石</h4><p>马云 + 企业家 - 互联网的结果是：<br><strong>王石</strong>、柳传志 …</p>
<p>马云 + 企业家的结果是：<br>创业者、<strong>李彦宏</strong> …</p>
<p>&emsp;&emsp;可见这个negative项有效地排除了互联网行业的企业家，有不错的实际效果。    </p>
<p>&emsp;&emsp;意义明确的输入很可能会获得理想的结果，例如法国+伦敦-英国=巴黎，类似的例子不胜枚举。</p>
<hr>
<h4 id="7-1-3-3-抽象的例子：神庙-工业-宗教-钢结构"><a href="#7-1-3-3-抽象的例子：神庙-工业-宗教-钢结构" class="headerlink" title="7.1.3.3 抽象的例子：神庙 + 工业 - 宗教 = 钢结构"></a>7.1.3.3 抽象的例子：神庙 + 工业 - 宗教 = 钢结构</h4><p>&emsp;&emsp;一位<strong>重视基建</strong>的印度领导人说过：”<strong>大坝</strong>是印度的<strong>神庙</strong>“，这是较高层次的抽象。        </p>
<p>神庙 + 工业 - 宗教的结果是：<br><strong>钢结构、厂房、工业园</strong>、模具、建材</p>
<p>&emsp;&emsp;虽然结果并没有出现大坝，但还是说得通的。    </p>
<p>&emsp;&emsp;在此列出神庙和工业的相似词作为对比：    </p>
<p>神庙：<br>帕特农、宫殿、城堡、神话故事、洋房</p>
<p>工业：<br>轻工、制造业、精细化工、纺织、工程技术</p>
<p>神庙+工业：<br>古村、成吉思汗、泰姬陵、古村落、柴达木盆地</p>
<p>可见<strong>每个参数都发挥了一定语义方面的作用</strong>。    </p>
<hr>
<h4 id="7-1-3-4-为什么会失败：火药桶-东亚-地区-欧洲-桥头堡"><a href="#7-1-3-4-为什么会失败：火药桶-东亚-地区-欧洲-桥头堡" class="headerlink" title="7.1.3.4 为什么会失败：火药桶 + 东亚 + 地区 - 欧洲 = 桥头堡"></a>7.1.3.4 为什么会失败：火药桶 + 东亚 + 地区 - 欧洲 = 桥头堡</h4><p>&emsp;&emsp;然而在抽象的例子中，<strong>失败的占了大多数</strong>，可能和文本的数量和预期结果判较为主观有关。</p>
<p>&emsp;&emsp;例如火药桶+东亚+地区-欧洲，预期的结果是朝鲜半岛，但给出的结果是桥头堡、信息港。再求火药桶和巴尔干的相似度，发现只有0.1。在语料内搜索，发现巴尔干火药桶这个词语组合出现频次很低。<br>&emsp;&emsp;可见需要得到更好的结果，就需要更多高质量的语料。</p>
<h3 id="7-1-4-扩充情感词典的试验"><a href="#7-1-4-扩充情感词典的试验" class="headerlink" title="7.1.4 扩充情感词典的试验"></a>7.1.4 扩充情感词典的试验</h3><p>&emsp;&emsp;根据后文一些论文提到的方法，本人尝试了<strong>用大连理工情感词库生成正面和负面情感词典</strong>。<br>&emsp;&emsp;实测时只输出对应词汇最相似的两个词，除了<strong>名词方面的扩充相关词容易出现问题外，结果大多都是正确的</strong>。但仍然需要<strong>人工检查</strong>生成的词汇。<br>&emsp;&emsp;另外，也会有<strong>身手不凡、坚持原则等词被划入负面</strong>，但很难说它们不是负面的。在极性词典中，<strong>“重点”这个词语被标注为带有“害怕”情感的词语</strong>，如果需要开展这方面的研究，需要有更权威的专业支持。    </p>
<p>　</p>
<h3 id="7-1-5-相似文本生成"><a href="#7-1-5-相似文本生成" class="headerlink" title="7.1.5 相似文本生成"></a>7.1.5 相似文本生成</h3><p>&emsp;&emsp;最近有一篇AI方面的新闻，主题是“微软AI小冰作现代诗”。受此启发，本人尝试生成相似的文本。<br>&emsp;&emsp;生成的方式是，将模板分词的同时做好词性标注，替换特定词性的词语，再将它们拼接起来。<br>&emsp;&emsp;做法比较简单，效果也一般，实际意义也不大。<br>&emsp;&emsp;如果不限定词性，甚至不限定相似词的字数，文章很容易变成前言不搭后语的崩坏作品。<br>&emsp;&emsp;但假如在有监督选词的情况下，例如进一步开发作为<strong>写作辅助工具</strong>，预计可以取得不错的效果。<br>&emsp;&emsp;其实，已经有类似的论文发表，具体的方式是<strong>将相似词推荐算法和输入法集成</strong>。    </p>
<p>结果一例：</p>
<p>依据p –&gt;  根据<br>坚持v –&gt;  秉持<br>引领v –&gt;  引导<br>抓手v –&gt;  突破口<br>推进v –&gt;  推动<br>完善v –&gt;  完备<br>按照p –&gt;  依照    </p>
<p>结果：<br><strong>根据</strong>学校学科发展规划，<strong>秉持</strong>学科<strong>引导</strong>，以教师教育学科和面向世界城市发展的特色学科专业群建设为<strong>突破口</strong>，<strong>推动</strong>学科专业布局调整，<strong>完备</strong>学科评价体系，<strong>依照</strong>“扶需、扶特、扶强”原则，大力加强重点学科建设。</p>
<p>原文：<br>依据学校学科发展规划，坚持学科引领，以教师教育学科和面向世界城市发展的特色学科专业群建设为抓手，推进学科专业布局调整，完善学科评价体系，按照“扶需、扶特、扶强”原则，大力加强重点学科建设。</p>
<h2 id="7-2-数据相关"><a href="#7-2-数据相关" class="headerlink" title="7.2 数据相关"></a>7.2 数据相关</h2><h3 id="7-2-1-爬虫"><a href="#7-2-1-爬虫" class="headerlink" title="7.2.1 爬虫"></a>7.2.1 爬虫</h3><p>&emsp;&emsp;本次实验采用了Python Scrapy作爬虫，并使用XPATH最大限度去除了Header、Footer等与正文无关的元素。但由于校园网CMS系统建设年代较远，DOM、ID和CLASS命名习惯都未遵守HTML5或Web 2.0相关的规范，实际执行起来效果一般，难以避免文本噪音。<br>&emsp;&emsp;除了不规范DOM的问题，还遇到了严重的超链错误问题。在一些分站，每次点击分页器都会直接在当前URL后APPEND新的参数，产生一个新的URL，爬虫本质上是个递归过程，因为这类网站会导致无限递归，所以没法完全爬完。<br>&emsp;&emsp;本次爬虫采用了两个初始入口：校园网首页和信息办的运行cms展示栏目，应该得以覆盖大部分页面。    </p>
<h3 id="7-2-2-文本排重"><a href="#7-2-2-文本排重" class="headerlink" title="7.2.2 文本排重"></a>7.2.2 文本排重</h3><p>&emsp;&emsp;由于校园网URL的特性，例如网站样式皮肤的路径附带在链接里，所以存在大量一文多链的情况，且由于不同皮肤带来的DOM改变，爬虫往往会爬到极为相似但不完全相同的文本。<br>&emsp;&emsp;在爬取数据后，本次实验使用了Python gensim库的docsim算法删除相似度大于99.99%的文章。</p>
<h3 id="7-2-3-分类情况"><a href="#7-2-3-分类情况" class="headerlink" title="7.2.3 分类情况"></a>7.2.3 分类情况</h3><h4 id="7-2-3-1-校内大理科类"><a href="#7-2-3-1-校内大理科类" class="headerlink" title="7.2.3.1 校内大理科类"></a>7.2.3.1 校内大理科类</h4><p>商学院、生环学院、信机学院、数理学院、金融学院、建工学院、化学实验教学示范中心、资源化学教育部重点实验室</p>
<h4 id="7-2-3-2-校内大文科类"><a href="#7-2-3-2-校内大文科类" class="headerlink" title="7.2.3.2 校内大文科类"></a>7.2.3.2 校内大文科类</h4><p>陶行知研究中心、人传学院、谢晋影视学院、马克思主义学院、非洲研究中心、对外汉语学院、国际与比较教育研究院、哲学学院、基础教育发展中心、美术学院、外语学院、法政学院、中国传统思想研究所</p>
<h4 id="7-2-3-3-外校大文科类"><a href="#7-2-3-3-外校大文科类" class="headerlink" title="7.2.3.3 外校大文科类"></a>7.2.3.3 外校大文科类</h4><p>华东师范大学中文系、华东师范大学哲学系、华东师范大学传播学院</p>
<h4 id="7-2-3-4-外校大理科类"><a href="#7-2-3-4-外校大理科类" class="headerlink" title="7.2.3.4 外校大理科类"></a>7.2.3.4 外校大理科类</h4><p>海事大学信工学院、华东理工大学化学与分子工程学院、上海大学生命科学学院</p>
<h2 id="7-3-有关Word2Vec应用的中文论文"><a href="#7-3-有关Word2Vec应用的中文论文" class="headerlink" title="7.3 有关Word2Vec应用的中文论文"></a>7.3 有关Word2Vec应用的中文论文</h2><p>《基于Word Embedding语义相似度的字母缩略术语消歧》 于东 荀恩东 北京语言大学汉语国际教育技术研发中心北京语言大学信息科学学院<br>《基于Word2Vec及大众健康信息源的疾病关联探测》 罗文馨 陈翀 邓思艺 北京师范大学政府管理学院<br>《基于Word2Vec的微博情感新词识别与倾向判断研究》隋浩 广西大学<br>《基于word2vec扩充情感词典的商品评论倾向分析》 陆峰 华南理工大学数学学院<br>《基于LDA和Word2Vec的推荐算法研究》 北京邮电大学 董文   </p>
<h2 id="7-4-未列出的实验内容"><a href="#7-4-未列出的实验内容" class="headerlink" title="7.4 未列出的实验内容"></a>7.4 未列出的实验内容</h2><p>&emsp;&emsp;另实验了Doc2Vec和支持向量机、Word2Vec和朴素贝叶斯结合等分类方式，但效果不理想，不予列出。<br>&emsp;&emsp;数据中的朴素贝叶斯分类器均采用伯努利模型，多项式、高斯判别分析模型表现均不如伯努利模型，不予列出。<br>&emsp;&emsp;数据中的支持向量机分类器均采用线性核，其他核（例如rbf核）的表现几乎都不如线性核，不予列出。     </p>
<h2 id="7-5-所用模型和算法的实现"><a href="#7-5-所用模型和算法的实现" class="headerlink" title="7.5 所用模型和算法的实现"></a>7.5 所用模型和算法的实现</h2><p>&emsp;&emsp;Word2Vec采用Python gensim库的Skip-Gram实现。<br>&emsp;&emsp;分类器、除Word2Vec外的特征提取工具、测试/训练集分类统计工具采用Python scikit-learn库的对应实现。<br>&emsp;&emsp;分词工具是jieba，除了开启了新词发现（HMM）功能外，其余配置默认。</p>

            </div>
          
           
            <div class="copyright">
                <div class="name">
                    <a>Author:</a>
                    <a>escapar</a>
                </div>
                <div class="link">
                    <a>Link:</a>
                    <a class="permalink" href="http://mns.re/2017/05/14/%E4%B8%8EWord2vec%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8E%A5%E8%A7%A6/">http://mns.re/2017/05/14/%E4%B8%8EWord2vec%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8E%A5%E8%A7%A6/</a>
                </div>
                <div class="license">
                    <a>License:</a>
                    <a>Strictly Follow CC BY-NC-SA 4.0.</a>
                </div>
            </div>
            

          


</article>


<div class="tip">
<button class="tip-btn" onclick="tipClick()">
    🍺
</button>
<div class="tip-img">
<ul>
    
<li>
    <img src="/img/qrcode1.jpg"></img>
</li>

 
<li>
    <img src="/img/qrcode2.jpg"></img>
</li>

</ul>
</div>
</div>

<script>
    function tipClick(){

        var element = document.querySelectorAll('.tip-img')[0];
        var flex= getComputedStyle(element)['display'] == 'flex'
        if(flex){
            element.style.display = 'none';
        }else{
            element.style.display = 'flex';
        }
        
    }
</script>

<div class="more">

    <div class="prev">
   <a href="/2020/03/16/adieu-world/">  残酷底层物语：你在底层研究生专业会体验到什么</a>
    </div>

<div></div>

    <div class="next">
    <a href="/2017/05/02/%E5%9B%9E%E6%9C%9B%E8%80%83%E7%A0%94%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E4%B8%8D%E5%BF%98%E5%88%9D%E5%BF%83/"> 回望考研（三）：不忘初心 </a>
    </div>
    
</div>

<div class="bdsharebuttonbox">
<a href="#" class="bds_weixin fa fa-weixin" data-cmd="weixin" title="分享到微信" style="color:#1cbd8f">
</a>
<a href="#" class="bds_tsina fa fa-weibo" data-cmd="tsina" title="分享到新浪微博" style="color:#ff6363">
</a>
<a href="#" class="bds_twi fa fa-twitter" data-cmd="twi" title="分享到Twitter" style="color:#00A7EB">
</a>
<a href="#" class="bds_fbook fa fa-facebook" data-cmd="fbook" title="分享到Facebook" style="color:#00A7EB">
</a>
</div>
<script>
window._bd_share_config={
    "common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},
    "share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='../../../../../static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>
 
<div id="comments">
  <div id="gitalk-container"></div>


</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '2dfbb34e289d8a3c1f8b',
        clientSecret: 'b47955416d6d4c310c9f125188b021b5c9e5e49c',
        id: window.location.pathname,
        repo: 'moonshine',
        owner: 'escapar',
        admin: 'escapar',
        createIssueManually: true,
        distractionFreeMode: false,
        language: 'en'
    })
    gitalk.render('gitalk-container')
</script>
    </main>
    <a class="not-found">not found!</a>
    <div class="search-items">
    </div>
    <a href="#header" id="top" style="display:none">
        <i class="fa fa-sort-asc fa-2x"></i>
    </a>
    <footer class="footer">
    <div class="footer-copyright">Powered by <a href="https://hexo.io"  class="link" target="_blank">Hexo</a> and
    <a href="//github.com/Vevlins/toki" class="link" target="_blank">Toki</a>.
    </div>
</footer>

    <!-- 
<script src="/js/jquery.js"></script>
 -->
    <!-- 
<script src="/js/toki.js"></script>
   -->
  <script>hljs.initHighlightingOnLoad();</script>
</body>
</html>